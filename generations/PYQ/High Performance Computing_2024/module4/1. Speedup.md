Overview:
# High Performance Computing: Speedup
Detailed Explanation:

## Overview:
High Performance Computing (HPC) is a field of study that focuses on the development and utilization of computing systems which operate at extremely high speeds, handling billions or even trillions of operations per second. One of the key metrics in HPC is **Speedup**. Speedup is the factor by which a parallel algorithm is faster than the corresponding sequential algorithm.

Speedup can be calculated using the formula: 

**Speedup = Time taken by the sequential algorithm / Time taken by the parallel algorithm**

The ideal speedup is linear, i.e., doubling the number of processing elements should halve the time to solution. However, due to various factors like communication overhead, load imbalance, etc, the actual speedup achieved is often less than the ideal speedup.

## Details:

### 1. **Types of Speedup:**

#### 1.1. **Absolute Speedup:**
   - Absolute speedup refers to the speedup achieved by a parallel system over the best sequential algorithm for the same problem.
   - **Formula: S<sub>Abs</sub> = T<sub>1</sub> / T<sub>p</sub>**
     - Where T<sub>1</sub> is the time taken by the best sequential algorithm and T<sub>p</sub> is the time taken by the parallel algorithm.

#### 1.2. **Relative Speedup:**
   - Relative speedup refers to the speedup achieved by a parallel system over the sequential algorithm that was actually parallelized.
   - **Formula: S<sub>Rel</sub> = T<sub>Seq</sub> / T<sub>p</sub>**
     - Where T<sub>Seq</sub> is the time taken by the sequential algorithm that was parallelized and T<sub>p</sub> is the time taken by the parallel algorithm.

### 2. **Amdahl's Law:**
- Amdahl's law is a formula which gives the maximum improvement to an overall system when only part of the system is improved. It is often used in parallel computing to predict the theoretical speedup when using multiple processors.
- **Formula: S<sub>p</sub> = 1 / ( (1-P) + (P/p) )**
  - Where P is the proportion of the program that can be made parallel and p is the number of processors.

### 3. **Gustafson’s Law:**
- Gustafson’s Law is an enhancement of Amdahl’s Law. It removes the limitation of the fixed problem size and promotes the execution of large tasks on parallel computing platforms.
- **Formula: S<sub>p</sub> = p - α * (p - 1)**
  - Where α is the non-parallel portion of process and p is the number of processors.

### 4. **Scalability:**
- Scalability is the ability of a parallel system to demonstrate a proportional increase in parallel speedup with the addition of more resources.
- Types of Scalability:
  - **Linear Scalability:** If the speedup is directly proportional to the number of processors, the system is said to have linear scalability.
  - **Super-linear Scalability:** If the speedup is more than the number of processors, the system is said to have super-linear scalability. This is rare and can happen when data fits into cache better in the parallel algorithm than the sequential one.
  - **Sub-linear Scalability:** If the speedup is less than the number of processors, the system is said to have sub-linear scalability. This is the most common case due to overheads in the parallel system.
  
## Conclusion:
Understanding and calculating speedup is crucial in High Performance Computing. It helps in evaluating the performance of parallel systems and algorithms, and provides insights for further optimization.