Overview:
# High Performance Computing: Building Blocks of MPI
Detailed Explanation:

## Overview:

The Message Passing Interface (MPI) is a standardized and portable message-passing standard designed by a group of researchers from academia and industry to function on a wide variety of parallel computing architectures. It is a method used to exchange information between the various processes of a program running on a distributed memory system. The primary building blocks of MPI are MPI Processes, MPI Communicators, MPI Point-to-Point communication, MPI Collective communication, and MPI Derived datatypes.

## Details:

### **1. MPI Processes:**

In MPI, a process is the execution of a program. Each MPI process has its own local variables, and each process operates independently. The processes can communicate with each other by sending and receiving messages.

### **2. MPI Communicators:**

MPI uses objects called communicators to define which group of processes has the ability to communicate with each other. The communicator encapsulates the context of a communication. The default communicator, `MPI_COMM_WORLD`, includes all the processes.

### **3. MPI Point-to-Point Communication:**

Point-to-Point communication involves two processes: a sender and a receiver. The fundamental functions are `MPI_Send` and `MPI_Recv`. 

- `MPI_Send`: It is used to send a message from one process to another.
- `MPI_Recv`: It is used to receive the sent message.

### **4. MPI Collective Communication:**

Collective communication is defined as communication between a group of processes that belong to a communicator. The main functions include `MPI_Bcast`, `MPI_Scatter`, `MPI_Gather`, and `MPI_Reduce`.

- `MPI_Bcast`: It is used to broadcast a message from the process with rank "root" to all other processes in the communicator.
- `MPI_Scatter`: It is used to distribute an array of data from one process (the root) to every process within a communicator.
- `MPI_Gather`: It is the opposite of `MPI_Scatter`. It gathers an array of data from all processes within a communicator to a single process (the root).
- `MPI_Reduce`: It applies a reduction operation (like sum, max, min, etc.) on all processes within a communicator and returns the result to the root process.

### **5. MPI Derived Datatypes:**

MPI provides a set of predefined datatypes for integers, real numbers, complex numbers, etc. But in some cases, we need to send complex data structures. MPI allows us to define our own datatypes, known as derived datatypes. This can be done using `MPI_Type_contiguous`, `MPI_Type_vector`, and `MPI_Type_struct` functions.

## Example:

Here is an example of a simple MPI program:

```C
#include <mpi.h>
#include <stdio.h>

int main(int argc, char** argv) {
    MPI_Init(NULL, NULL);

    int world_size;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);

    int world_rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

    char processor_name[MPI_MAX_PROCESSOR_NAME];
    int name_len;
    MPI_Get_processor_name(processor_name, &name_len);

    printf("Hello world from processor %s, rank %d out of %d processors\n",
           processor_name, world_rank, world_size);

    MPI_Finalize();
}
```

In this program, `MPI_Init` initializes the MPI environment, `MPI_Comm_size` gets the number of processes, `MPI_Comm_rank` gets the rank of the process, `MPI_Get_processor_name` gets the name of the processor, and `MPI_Finalize` ends the MPI environment.