Syllabus:

Module I:
1. Explain levels of parallelism.
2. Explain Flynn’s architectural scheme.
3. Explain Pipelining and Superscalar Execution with example.
4. Explain various parameters to evaluate memory system performance and different approaches to hide memory latency.
5. Explain Amdahl’s Law and solve the following problem: 95% of a program’s execution time occurs inside a loop that can be executed in parallel. What is the maximum speedup expected from a parallel version of the program executing on 8 CPUs?

Module II:
1. Compare the buffered blocking message passing operation with the non-buffered blocking message passing operation.
2. Compare MPI_Send and MPI_Recv.
3. Explain recursive decomposition technique with example.
4. Explain exploratory decomposition technique with example.
5. Explain OpenMP Programming Model with thread implementation.

Module III:
1. Explain Pipelining and Superscalar Execution.
2. Explain Flynn's Classification model.
3. Explain Feng's Classification model.
4. Explain MPI routines.
5. Architecture of an Ideal Parallel Computer.
6. Explain principal parameters that determine the communication latency.
7. Explain the Building BLocks of MPI.
8. Explain recursive decomposition and data-decomposition.
9. Explain exploratory decomposition and speculative decomposition.
10. Explain the applications of High Performance Computing.
11. Compare Data-Parallel Model and Task Graph Model.
12. Compare Work Pool Model and Master-Slave Model.
13. Explain Synchronization Primitives in Pthreads.
14. Define Speedup, execution time, and efficiency, cost and scalability with example.
15. Explain Amdahl’s law.